# -*- coding: utf-8 -*-
"""
Predictor Engine (Production) - Beast Mode (ÏïºÏàò Î™®Îìú)
- üî• DART Í≥µÏãú ÏôÑÏ†Ñ Ï†úÍ±∞ (Í≤ΩÎüâÌôî)
- SEC Form 4 (ÎØ∏Íµ≠ ÎÇ¥Î∂ÄÏûê Îß§Ïàò)
- SEC 13D/13G (Í≥†Îûò Ï∂îÏ†Å)
- Ï§ëÎ≥µ Î∞©ÏßÄ ÏôÑÎ≤Ω
"""

import asyncio
import logging
from datetime import datetime, timedelta
import aiohttp
from bs4 import BeautifulSoup
import re
import yfinance as yf

logger = logging.getLogger(__name__)

class PredictorEngine:
    def __init__(self):
        # üî• v3.0: DART API ÏôÑÏ†Ñ Ï†úÍ±∞
        # SEC (ÎØ∏Íµ≠)Îßå Ïú†ÏßÄ
        self.sec_form4_url = "https://www.sec.gov/cgi-bin/browse-edgar"
        self.sec_13d_url = "https://www.sec.gov/cgi-bin/browse-edgar"
        self.sec_company_tickers = "https://www.sec.gov/files/company_tickers.json"
        
        # Ï§ëÎ≥µ Î∞©ÏßÄ (SECÎßå)
        self.seen_form4 = set()
        self.seen_13d = set()
        
        # CIK ‚Üí Ìã∞Ïª§ Îß§Ìïë
        self.cik_to_ticker = {}
        
        # üêã Ïú†Î™Ö Í≥†Îûò (ÎØ∏Íµ≠) - 40Î™Ö
        self.famous_us_whales = {
            'ICAHN': 'üëë Carl Icahn',
            'ACKMAN': 'üëë Bill Ackman (Pershing)',
            'EINHORN': 'üëë David Einhorn',
            'BERKSHIRE': 'üèÜ Warren Buffett',
            'GATES': 'üèÜ Bill Gates',
            'SOROS': 'üèÜ George Soros',
            'STARBOARD': '‚öîÔ∏è Starboard Value',
            'ELLIOTT': '‚öîÔ∏è Elliott Management',
            'THIRD POINT': '‚öîÔ∏è Third Point',
            'PERSHING': '‚öîÔ∏è Pershing Square',
            'VALUEACT': '‚öîÔ∏è ValueAct',
            'JANA': '‚öîÔ∏è JANA Partners',
            'BLACKROCK': 'üè¶ BlackRock',
            'VANGUARD': 'üè¶ Vanguard',
            'STATE STREET': 'üè¶ State Street',
            'FIDELITY': 'üè¶ Fidelity',
            'GOLDMAN': 'üè¶ Goldman Sachs',
            'GOLDMAN SACHS': 'üè¶ Goldman Sachs',
            'MORGAN STANLEY': 'üè¶ Morgan Stanley',
            'JP MORGAN': 'üè¶ JP Morgan',
            'JPMORGAN': 'üè¶ JP Morgan',
            'CITADEL': 'ü§ñ Citadel',
            'RENAISSANCE': 'ü§ñ Renaissance Tech',
            'BRIDGEWATER': 'ü§ñ Bridgewater',
            'TWO SIGMA': 'ü§ñ Two Sigma',
            'DE SHAW': 'ü§ñ D.E. Shaw',
            'MILLENNIUM': 'ü§ñ Millennium',
            'SOFTBANK': 'üáØüáµ SoftBank (ÏÜêÏ†ïÏùò)',
            'BAUPOST': 'üíé Baupost',
            'APPALOOSA': 'üíé Appaloosa',
            'GREENLIGHT': 'üíé Greenlight',
            'LONE PINE': 'üíé Lone Pine',
        }
        
        logger.info("üîÆ Predictor Engine (Production) Beast Mode Ï¥àÍ∏∞Ìôî (SEC Only)")
    
    async def generate_daily_report(self, market='US'):
        """
        ÏïÑÏπ®/Ï†ÄÎÖÅ Î¶¨Ìè¨Ìä∏ (Í∞ÑÏÜåÌôî)
        v3.0: SEC Í≥µÏãúÎßå Ìè¨Ìï®
        """
        today = datetime.now().date()
        
        report = {
            'date': today,
            'market': market,
            'hot_stocks': [],
            'events_today': [],
            'risks': []
        }
        
        if market == 'US':
            # SEC Form 4 + 13D/13G
            form4_signals = await self.scan_sec_form4(hours=24)
            filing_13d = await self.scan_sec_13d(hours=24)
            
            all_signals = form4_signals + filing_13d
            report['events_today'] = self._deduplicate_and_rank(all_signals)
            
            # Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨
            report['risks'] = await self.check_market_risks('US')
        
        logger.info(f"üìä ÏùºÏùº Î¶¨Ìè¨Ìä∏: {len(report['events_today'])}Í±¥")
        return report
    
    async def scan_sec_form4(self, hours=24):
        """
        ÎØ∏Íµ≠ SEC Form 4 (ÎÇ¥Î∂ÄÏûê Í±∞Îûò)
        v3.0: Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ
        """
        signals = []
        
        try:
            params = {
                'action': 'getcurrent',
                'type': '4',
                'company': '',
                'dateb': '',
                'owner': 'include',
                'start': '0',
                'count': '100',
                'output': 'atom'
            }
            
            headers = {'User-Agent': 'Mozilla/5.0 (PredictorBot/3.0)'}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.sec_form4_url, params=params, headers=headers, timeout=15) as response:
                    if response.status != 200:
                        logger.warning(f"Form 4 Ï†ëÍ∑º Ïã§Ìå®: {response.status}")
                        return signals
                    
                    xml = await response.text()
                    soup = BeautifulSoup(xml, 'xml')
                    entries = soup.find_all('entry')[:40]
                    
                    for entry in entries:
                        try:
                            title = entry.find('title').text
                            link = entry.find('link')['href']
                            updated = entry.find('updated').text
                            
                            if link in self.seen_form4:
                                continue
                            
                            filing_time = datetime.fromisoformat(updated.replace('Z', '+00:00'))
                            now = datetime.now(filing_time.tzinfo)
                            
                            if (now - filing_time).total_seconds() > hours * 3600:
                                continue
                            
                            ticker_match = re.search(r'\(([A-Z]{1,5})\)', title)
                            if not ticker_match:
                                continue
                            
                            ticker = ticker_match.group(1)
                            
                            # Îß§Ïàò/Îß§ÎèÑ Íµ¨Î∂Ñ
                            transaction_type = await self._parse_form4_type(link, session)
                            
                            if transaction_type != 'BUY':
                                continue
                            
                            self.seen_form4.add(link)
                            
                            signals.append({
                                'ticker': ticker,
                                'name': ticker,
                                'signal_type': 'insider_buy',
                                'event_date': filing_time.date(),
                                'confidence': 0.80,
                                'expected_impact': '+10~30%',
                                'reason': 'üëî ÏûÑÏõê Îß§Ïàò (Form 4)',
                                'filing_id': link,
                                'market': 'US',
                                'details': {
                                    'filing_url': link,
                                    'transaction_type': transaction_type
                                }
                            })
                            
                            logger.info(f"üëî Form 4: {ticker} Îß§Ïàò")
                            
                        except Exception as e:
                            logger.debug(f"Form 4 Ìï≠Î™© Ïò§Î•ò: {e}")
                            continue
                    
                    if len(self.seen_form4) > 500:
                        self.seen_form4.clear()
            
            logger.info(f"‚úÖ Form 4: {len(signals)}Í±¥")
            return signals
            
        except Exception as e:
            logger.error(f"Form 4 Ïò§Î•ò: {e}")
            return signals
    
    async def scan_sec_13d(self, hours=24):
        """
        ÎØ∏Íµ≠ SEC 13D/13G (Í≥†Îûò Ï∂îÏ†Å)
        v3.0: Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ
        """
        signals = []
        
        try:
            # CIK Îß§Ìïë Î°úÎìú (ÏµúÏ¥à 1Ìöå)
            if not self.cik_to_ticker:
                await self._load_sec_mappings()
            
            params = {
                'action': 'getcurrent',
                'type': '',
                'company': '',
                'dateb': '',
                'owner': 'include',
                'start': '0',
                'count': '100',
                'output': 'atom'
            }
            
            headers = {
                'User-Agent': 'StockAlertBot admin@stockbot.com',
                'Accept-Encoding': 'gzip, deflate',
                'Host': 'www.sec.gov'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.sec_13d_url, params=params, headers=headers, timeout=20) as response:
                    if response.status != 200:
                        logger.warning(f"13D/13G Ï†ëÍ∑º Ïã§Ìå®: {response.status}")
                        return signals
                    
                    xml = await response.text()
                    soup = BeautifulSoup(xml, 'xml')
                    entries = soup.find_all('entry')[:50]
                    
                    for entry in entries:
                        try:
                            title = entry.find('title').text
                            link = entry.find('link')['href']
                            updated = entry.find('updated').text
                            
                            summary_tag = entry.find('summary')
                            summary = summary_tag.text if summary_tag else ''
                            
                            if link in self.seen_13d:
                                continue
                            
                            try:
                                filing_time = datetime.fromisoformat(updated.replace('Z', '+00:00'))
                            except:
                                filing_time = datetime.now()
                            
                            now = datetime.now(filing_time.tzinfo if filing_time.tzinfo else None)
                            if (now - filing_time).total_seconds() > hours * 3600:
                                continue
                            
                            # 13D/13G ÌïÑÌÑ∞
                            form_type = None
                            priority = 0
                            
                            upper_title = title.upper()
                            
                            if "SC 13D/A" in upper_title:
                                form_type = "üî• SC 13D/A (ÏßÄÎ∂Ñ Î≥ÄÍ≤Ω)"
                                priority = 9
                            elif "SC 13D" in upper_title:
                                form_type = "üö® SC 13D (5%+ Í≥µÍ≤©Ï†Å Ìà¨Ïûê)"
                                priority = 10
                            elif "SC 13G/A" in upper_title:
                                form_type = "üìà SC 13G/A (ÏßÄÎ∂Ñ Î≥ÄÍ≤Ω)"
                                priority = 6
                            elif "SC 13G" in upper_title:
                                form_type = "üìä SC 13G (5%+ Îã®Ïàú Ìà¨Ïûê)"
                                priority = 7
                            else:
                                continue
                            
                            # Ìã∞Ïª§ Ï∂îÏ∂ú
                            ticker = await self._extract_ticker_multi(title, summary, link, session)
                            final_symbol = ticker if ticker else "UNKNOWN"
                            
                            # Í≥†Îûò ÌôïÏù∏
                            whale_name = None
                            for whale_key, whale_desc in self.famous_us_whales.items():
                                if whale_key in title.upper() or whale_key in summary.upper():
                                    whale_name = whale_desc
                                    priority += 3
                                    break
                            
                            self.seen_13d.add(link)
                            
                            trigger_msg = form_type
                            if whale_name:
                                trigger_msg = f"{whale_name}\n{form_type}"
                            
                            signals.append({
                                'ticker': final_symbol,
                                'name': final_symbol,
                                'signal_type': 'whale_alert',
                                'event_date': filing_time.date(),
                                'confidence': 0.85,
                                'expected_impact': '+15~50%',
                                'reason': trigger_msg,
                                'filing_id': link,
                                'market': 'US',
                                'details': {
                                    'filing_url': link,
                                    'whale_name': whale_name,
                                    'form_type': form_type
                                }
                            })
                            
                            logger.info(f"üêã 13D: {final_symbol} - {form_type}")
                            
                        except Exception as e:
                            logger.debug(f"13D Ìï≠Î™© Ïò§Î•ò: {e}")
                            continue
                    
                    if len(self.seen_13d) > 1000:
                        self.seen_13d.clear()
            
            logger.info(f"‚úÖ 13D/13G: {len(signals)}Í±¥")
            return signals
            
        except Exception as e:
            logger.error(f"13D/13G Ïò§Î•ò: {e}")
            return signals
    
    async def _load_sec_mappings(self):
        """SEC CIK ‚Üí Ìã∞Ïª§ Îß§Ìïë"""
        try:
            headers = {'User-Agent': 'StockAlertBot admin@stockbot.com'}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.sec_company_tickers, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        for company in data.values():
                            cik = str(company['cik_str']).zfill(10)
                            ticker = company['ticker']
                            self.cik_to_ticker[cik] = ticker
                        
                        logger.info(f"‚úÖ SEC Îß§Ìïë: {len(self.cik_to_ticker)}Í∞ú")
                    
        except Exception as e:
            logger.error(f"SEC Îß§Ìïë Ïò§Î•ò: {e}")
    
    async def _extract_ticker_multi(self, title, summary, link, session):
        """Îã§Ï§ë Ï†ÑÎûµ Ìã∞Ïª§ Ï∂îÏ∂ú"""
        # Ï†ÑÎûµ 1: CIK
        cik_match = re.search(r'\((\d{10})\)', title)
        if not cik_match:
            cik_match = re.search(r'\((\d{7,10})\)', title)
        
        if cik_match:
            cik = cik_match.group(1).zfill(10)
            ticker = self.cik_to_ticker.get(cik)
            if ticker:
                return ticker
        
        # Ï†ÑÎûµ 2: URL CIK
        if link:
            url_cik_match = re.search(r'/data/(\d+)/', link)
            if url_cik_match:
                cik = url_cik_match.group(1).zfill(10)
                ticker = self.cik_to_ticker.get(cik)
                if ticker:
                    return ticker
        
        # Ï†ÑÎûµ 3: Í¥ÑÌò∏ Ìã∞Ïª§
        ticker_match = re.search(r'\(([A-Z]{1,5})\)', title)
        if ticker_match:
            return ticker_match.group(1)
        
        return None
    
    async def _parse_form4_type(self, filing_url, session):
        """Form 4 Îß§Ïàò/Îß§ÎèÑ Íµ¨Î∂Ñ"""
        try:
            async with session.get(filing_url, timeout=5) as response:
                if response.status != 200:
                    return 'UNKNOWN'
                
                xml_text = await response.text()
                
                if '<transactionCode>P</transactionCode>' in xml_text:
                    return 'BUY'
                elif '<transactionCode>S</transactionCode>' in xml_text:
                    return 'SELL'
                else:
                    return 'UNKNOWN'
        except:
            return 'UNKNOWN'
    
    async def check_market_risks(self, market):
        """Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨"""
        risks = []
        
        try:
            if market == 'US':
                vix = yf.Ticker('^VIX')
                vix_hist = vix.history(period='1d')
                
                if not vix_hist.empty:
                    vix_value = vix_hist['Close'].iloc[-1]
                    if vix_value > 30:
                        risks.append(f"‚ö†Ô∏è VIX Í≥†Í≥µÌñâÏßÑ ({vix_value:.1f})")
                    elif vix_value > 20:
                        risks.append(f"üìä VIX ÏÉÅÏäπ ({vix_value:.1f})")
                
                sp500 = yf.Ticker('^GSPC')
                sp_hist = sp500.history(period='5d')
                
                if len(sp_hist) >= 2:
                    change = ((sp_hist['Close'].iloc[-1] - sp_hist['Close'].iloc[-2]) / sp_hist['Close'].iloc[-2]) * 100
                    if change < -2:
                        risks.append(f"üî¥ S&P 500 Í∏âÎùΩ ({change:.1f}%)")
            
            elif market == 'KR':
                kospi = yf.Ticker('^KS11')
                kospi_hist = kospi.history(period='5d')
                
                if len(kospi_hist) >= 2:
                    change = ((kospi_hist['Close'].iloc[-1] - kospi_hist['Close'].iloc[-2]) / kospi_hist['Close'].iloc[-2]) * 100
                    if change < -2:
                        risks.append(f"üî¥ KOSPI Í∏âÎùΩ ({change:.1f}%)")
        
        except Exception as e:
            logger.debug(f"Î¶¨Ïä§ÌÅ¨ Ï≤¥ÌÅ¨ Ïò§Î•ò: {e}")
        
        return risks
    
    def _deduplicate_and_rank(self, signals):
        """
        Ï§ëÎ≥µ Ï†úÍ±∞ & ÏàúÏúÑ
        """
        unique_map = {}
        
        for signal in signals:
            ticker = signal.get('ticker', 'UNKNOWN')
            name = signal.get('name', 'Unknown')
            filing_id = signal.get('filing_id', '')
            
            # UNKNOWNÏù¥Î©¥ ÌöåÏÇ¨Î™ÖÏúºÎ°ú Íµ¨Î∂Ñ
            if ticker == 'UNKNOWN' or not ticker:
                unique_key = f"UNKNOWN_{name}"
            else:
                unique_key = ticker
            
            # Í≥†Ïú† ID = unique_key + filing_id
            signal_id = f"{unique_key}_{filing_id}"
            
            # ÏßÑÏßú Ï§ëÎ≥µ(=Í∞ôÏùÄ Í≥µÏãú)Îßå Ï†úÏô∏
            if signal_id not in unique_map:
                unique_map[signal_id] = signal
        
        # Ïã†Î¢∞ÎèÑ Ïàú Ï†ïÎ†¨
        ranked = sorted(
            unique_map.values(),
            key=lambda x: x.get('confidence', 0),
            reverse=True
        )
        
        return ranked[:10]  # TOP 10
