# -*- coding: utf-8 -*-
"""
News Engine v2.2 - v3.0 ì—…ê·¸ë ˆì´ë“œ (í˜¸í™˜ì„± ìœ ì§€)
- íŒŒì¼ëª…: v2_2 (í˜¸í™˜ì„±)
- ë‚´ìš©ë¬¼: v3.0 (ìµœì‹ )
- 5ëŒ€ì¥ ë‰´ìŠ¤ ì†ŒìŠ¤ + SEC 8-K
- curl_cffi ë³´ì•ˆ ìš°íšŒ
- KST ì‹œê°„ ì²˜ë¦¬
- AI ëª¨ë¸ëª… ì¶”ì 
"""

import asyncio
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import feedparser
import pytz
from difflib import SequenceMatcher
import re
from curl_cffi.requests import AsyncSession

from ai_brain_v2_2 import AIBrainV2_2
from config import Config

logger = logging.getLogger(__name__)

class NewsEngineV2_2:  # ğŸ”¥ í´ë˜ìŠ¤ëª… v2_2 ìœ ì§€!
    def __init__(self, ai_brain):
        self.ai = ai_brain
        self.seen_urls = set()
        self.seen_titles = []
        
        # Timezone
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ğŸ†• 5ëŒ€ì¥ ë‰´ìŠ¤ ì†ŒìŠ¤ + SEC 8-K (v3.0)
        self.sources = [
            {
                'name': 'PR Newswire',
                'type': 'rss',
                'url': 'https://www.prnewswire.com/rss/news-releases-list.rss',
                'market': 'US'
            },
            {
                'name': 'GlobeNewswire',
                'type': 'rss',
                'url': 'https://www.globenewswire.com/RssFeed/subjectcode/15-allcategories/feedTitle/GlobeNewswire%20-%20All%20Categories',
                'market': 'US'
            },
            {
                'name': 'Business Wire',
                'type': 'html',
                'url': 'https://www.businesswire.com/portal/site/home/news/',
                'pattern': r'/news/home/\d+/',
                'market': 'US'
            },
            {
                'name': 'Benzinga',
                'type': 'html',
                'url': 'https://www.benzinga.com/news',
                'pattern': r'/news/\d+/',
                'market': 'US'
            },
        ]
        
        # SEC 8-K ê³µì‹œ
        self.sec_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=8-K&company=&dateb=&owner=include&start=0&count=100&output=atom'
        
        logger.info("ğŸ“° News Engine v2.2 (v3.0 ì—…ê·¸ë ˆì´ë“œ) ì´ˆê¸°í™”")
    
    async def scan_all_sources(self):
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ë³‘ë ¬ ìŠ¤ìº” (curl_cffi)"""
        async with AsyncSession(impersonate="chrome110") as session:
            tasks = []
            
            for source in self.sources:
                if source['type'] == 'rss':
                    tasks.append(self._fetch_rss(session, source))
                elif source['type'] == 'html':
                    tasks.append(self._fetch_html(session, source))
            
            tasks.append(self._fetch_sec(session))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            news_list = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    source_name = self.sources[i]['name'] if i < len(self.sources) else 'SEC 8-K'
                    logger.error(f"{source_name} ìŠ¤ìº” ì˜¤ë¥˜: {result}")
                elif result:
                    news_list.extend(result)
            
            news_list.sort(key=lambda x: x.get('published_timestamp', 0), reverse=True)
            
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘: {len(news_list)}ê°œ (5ëŒ€ì¥ + SEC)")
            return news_list
    
    async def _fetch_rss(self, session, source):
        """RSS í”¼ë“œ ìŠ¤ìº”"""
        items = []
        
        try:
            response = await session.get(source['url'], timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"{source['name']} RSS ì‹¤íŒ¨: {response.status_code}")
                return items
            
            feed = feedparser.parse(response.text)
            
            if not feed.entries:
                logger.warning(f"{source['name']} ì—”íŠ¸ë¦¬ ì—†ìŒ")
                return items
            
            for entry in feed.entries[:20]:
                try:
                    title = entry.title
                    link = entry.link
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    pub_time = self._extract_rss_time(entry)
                    
                    age_hours = (datetime.now(self.kst) - pub_time).total_seconds() / 3600
                    if age_hours > 24:
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"{source['name']}_{link}",
                        'title': title,
                        'url': link,
                        'source': source['name'],
                        'market': source['market'],
                        'type': 'news',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"RSS í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} RSS ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_html(self, session, source):
        """HTML í¬ë¡¤ë§ (Golden Logic)"""
        items = []
        
        try:
            headers = {'Referer': 'https://www.google.com/'}
            response = await session.get(source['url'], headers=headers, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"{source['name']} HTML ì‹¤íŒ¨: {response.status_code}")
                return items
            
            soup = BeautifulSoup(response.text, 'lxml')
            links = soup.find_all('a', href=re.compile(source['pattern']))
            
            for link_tag in links[:15]:
                try:
                    title = link_tag.get_text(strip=True)
                    link = link_tag.get('href')
                    
                    if not link.startswith('http'):
                        if source['name'] == 'Business Wire':
                            link = 'https://www.businesswire.com' + link
                        elif source['name'] == 'Benzinga':
                            link = 'https://www.benzinga.com' + link
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    pub_time = datetime.now(self.kst)
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"{source['name']}_{link}",
                        'title': title,
                        'url': link,
                        'source': source['name'],
                        'market': source['market'],
                        'type': 'news',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"HTML ë§í¬ ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} HTML ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_sec(self, session):
        """SEC 8-K ê³µì‹œ í¬ë¡¤ë§"""
        items = []
        
        try:
            headers = {'User-Agent': 'StockBot/3.0 (admin@stockbot.com)'}
            response = await session.get(self.sec_url, headers=headers, timeout=20)
            
            if response.status_code != 200:
                logger.warning(f"SEC 8-K ì‹¤íŒ¨: {response.status_code}")
                return items
            
            soup = BeautifulSoup(response.text, 'xml')
            entries = soup.find_all('entry')
            
            for entry in entries[:30]:
                try:
                    title_tag = entry.find('title')
                    link_tag = entry.find('link')
                    updated_tag = entry.find('updated')
                    
                    if not title_tag or not link_tag:
                        continue
                    
                    title = title_tag.text.strip()
                    link = link_tag.get('href')
                    
                    title = f"[ê³µì‹œ] {title}"
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    pub_time = self._extract_sec_time(updated_tag)
                    
                    age_hours = (datetime.now(self.kst) - pub_time).total_seconds() / 3600
                    if age_hours > 24:
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"SEC_{link}",
                        'title': title,
                        'url': link,
                        'source': 'SEC 8-K',
                        'market': 'US',
                        'type': 'filing',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"SEC í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… SEC 8-K: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"SEC 8-K ì˜¤ë¥˜: {e}")
            return items
    
    def _extract_rss_time(self, entry):
        """RSS ë°œê°„ ì‹œê°„ íŒŒì‹± â†’ KST"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                t = entry.published_parsed
                dt_naive = datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
                dt_kst = dt_naive + timedelta(hours=9)
                return self.kst.localize(dt_kst)
            
            if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                t = entry.updated_parsed
                dt_naive = datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
                dt_kst = dt_naive + timedelta(hours=9)
                return self.kst.localize(dt_kst)
                
        except Exception as e:
            logger.debug(f"RSS ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return datetime.now(self.kst)
    
    def _extract_sec_time(self, updated_tag):
        """SEC XML updated ì‹œê°„ íŒŒì‹± â†’ KST"""
        try:
            if updated_tag:
                time_str = updated_tag.text.strip()
                
                if time_str.endswith('Z'):
                    time_str = time_str.replace('Z', '+00:00')
                
                dt = datetime.fromisoformat(time_str)
                dt_kst = dt.astimezone(self.kst)
                
                return dt_kst
                
        except Exception as e:
            logger.debug(f"SEC ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return datetime.now(self.kst)
    
    def _is_duplicate(self, title, url):
        """ì¤‘ë³µ ì²´í¬"""
        if url in self.seen_urls:
            return True
        
        for seen_title in self.seen_titles:
            if SequenceMatcher(None, title, seen_title).ratio() > 0.85:
                return True
        
        return False
    
    def _register_news(self, title, url):
        """ì¤‘ë³µ ë°©ì§€ ë“±ë¡"""
        self.seen_urls.add(url)
        self.seen_titles.append(title)
        
        if len(self.seen_titles) > 100:
            self.seen_titles.pop(0)
    
    def _passes_keyword_filter(self, title):
        """í‚¤ì›Œë“œ í•„í„°"""
        title_lower = title.lower()
        
        has_positive = any(kw in title_lower for kw in Config.POSITIVE_KEYWORDS)
        has_negative = any(kw in title_lower for kw in Config.NEGATIVE_KEYWORDS)
        
        return has_positive and not has_negative
    
    async def process_news(self, news_item):
        """ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (SEC ê³µì‹œ ìµœì í™”)"""
        try:
            is_filing = news_item.get('type') == 'filing'
            
            threshold = 7.5 if is_filing else 8.0
            is_promising = await self.ai.quick_score(news_item['title'], threshold=threshold)
            
            if not is_promising:
                return None
            
            analysis = await self.ai.analyze_news_signal(news_item)
            
            if not analysis:
                return None
            
            if is_filing and analysis['score'] < 9.5:
                analysis['score'] = min(analysis['score'] + 0.5, 10.0)
                logger.info(f"ğŸ“‹ ê³µì‹œ ì ìˆ˜ ë³´ì •: {analysis['score']}")
            
            min_score = 8.0 if is_filing else 8.5
            if analysis['score'] < min_score:
                return None
            
            verified = await self.verify_signals(analysis, news_item)
            
            if not verified:
                return None
            
            return {
                'news': news_item,
                'analysis': analysis,
                'verified': True,
                'verification_details': verified,
                'model_used': analysis.get('model_used', 'unknown'),
                'is_filing': is_filing,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    async def verify_signals(self, analysis, news_item):
        """3ì¤‘ ê²€ì¦"""
        verification = {
            'ai_score': analysis['score'],
            'checks_passed': [],
            'total_score': 0
        }
        
        if analysis['score'] >= 9.0:
            verification['total_score'] += 50
            verification['checks_passed'].append('AI ì´ˆê³ ì ìˆ˜')
        elif analysis['score'] >= 8.5:
            verification['total_score'] += 40
            verification['checks_passed'].append('AI ê³ ì ìˆ˜')
        else:
            return None
        
        if analysis.get('certainty') == 'confirmed':
            verification['total_score'] += 15
            verification['checks_passed'].append('í™•ì • ë‰´ìŠ¤')
        
        if news_item.get('type') == 'filing':
            verification['total_score'] += 10
            verification['checks_passed'].append('SEC ê³µì‹ ê³µì‹œ')
        
        verification['total_score'] += 10
        verification['checks_passed'].append('ì‹œì¥ ë¶„ì„')
        
        news_type = self._classify_news_type(news_item['title'])
        pattern_score = {
            'approval': 25,
            'earnings': 20,
            'contract': 20,
            'government': 15,
            'product': 15,
            'other': 5
        }.get(news_type, 5)
        
        verification['total_score'] += pattern_score
        verification['checks_passed'].append(f'íƒ€ì…: {news_type}')
        
        if verification['total_score'] >= 80:
            return verification
        else:
            return None
    
    def _classify_news_type(self, title):
        """ë‰´ìŠ¤ íƒ€ì… ë¶„ë¥˜"""
        title_lower = title.lower()
        
        keywords = {
            'approval': ['ìŠ¹ì¸', 'approval', 'approved', 'fda'],
            'earnings': ['ì‹¤ì ', 'earnings', 'ì˜ì—…ì´ìµ'],
            'contract': ['ê³„ì•½', 'contract', 'ìˆ˜ì£¼'],
            'government': ['ì •ë¶€', 'government', 'subsidy'],
            'product': ['ì¶œì‹œ', 'launch', 'product'],
        }
        
        for news_type, words in keywords.items():
            if any(word in title_lower for word in words):
                return news_type
        
        return 'other'
    
    def cleanup_old_news(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if len(self.seen_urls) > 1000:
            self.seen_urls = set(list(self.seen_urls)[-500:])
        if len(self.seen_titles) > 100:
            self.seen_titles = self.seen_titles[-50:]
