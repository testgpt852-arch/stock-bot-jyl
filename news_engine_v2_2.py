# -*- coding: utf-8 -*-
"""
News Engine v2.2 - ì™„ì „ì²´
- ê¸°ì¡´ ëª¨ë“  ë…¸í•˜ìš° í†µí•©
- Business Wire ì¶”ê°€
- ë‰´ìŠ¤ ì†ŒìŠ¤ 6ê°œ
- ì¤‘ë³µ ë°©ì§€ ì™„ë²½
"""

import asyncio
import aiohttp
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import feedparser
import pytz
from difflib import SequenceMatcher
import re

from ai_brain_v2 import AIBrainV2_2
from config import Config

logger = logging.getLogger(__name__)

class NewsEngineV2_2:
    def __init__(self, ai_brain):
        self.ai = ai_brain
        self.seen_urls = set()
        self.seen_titles = []
        
        # Timezone
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ğŸ†• ë‰´ìŠ¤ ì†ŒìŠ¤ 6ê°œ (Business Wire ì¶”ê°€)
        self.sources = [
            {
                'name': 'Yahoo Finance',
                'type': 'rss',
                'url': 'https://finance.yahoo.com/news/rssindex',
                'market': 'US'
            },
            {
                'name': 'GlobeNewswire',
                'type': 'rss',
                'url': 'https://www.globenewswire.com/RssFeed',
                'market': 'US'
            },
            {
                'name': 'PR Newswire',
                'type': 'html',
                'url': 'https://www.prnewswire.com/news-releases/news-releases-list/',
                'base_url': 'https://www.prnewswire.com',
                'market': 'US'
            },
            {
                'name': 'Business Wire',
                'type': 'rss',
                'url': 'https://feeds.businesswire.com/businesswire/news',
                'market': 'US'
            },
            {
                'name': 'Marketwired',
                'type': 'rss',
                'url': 'https://www.marketwired.com/news_feed',
                'market': 'US'
            },
            {
                'name': 'AccessWire',
                'type': 'rss',
                'url': 'https://www.accesswire.com/newsroom/rss',
                'market': 'US'
            },
        ]
        
        logger.info("ğŸ“° News Engine v2.2 ì´ˆê¸°í™” (6ê°œ ì†ŒìŠ¤)")
    
    async def scan_all_sources(self):
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ë³‘ë ¬ ìŠ¤ìº”"""
        tasks = []
        
        for source in self.sources:
            if source['type'] == 'rss':
                tasks.append(self._fetch_rss(source))
            elif source['type'] == 'html':
                tasks.append(self._fetch_html(source))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        news_list = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"{self.sources[i]['name']} ìŠ¤ìº” ì˜¤ë¥˜: {result}")
            elif result:
                news_list.extend(result)
        
        # ì‹œê°„ìˆœ ì •ë ¬
        news_list.sort(key=lambda x: x.get('published_timestamp', 0), reverse=True)
        
        logger.info(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘: {len(news_list)}ê°œ (6ê°œ ì†ŒìŠ¤)")
        return news_list
    
    async def _fetch_rss(self, source):
        """RSS í”¼ë“œ ìŠ¤ìº”"""
        items = []
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(source['url'], headers=headers, timeout=10) as resp:
                    if resp.status != 200:
                        logger.warning(f"{source['name']} RSS ì‹¤íŒ¨: {resp.status}")
                        return items
                    
                    feed = feedparser.parse(await resp.text())
                    
                    if not feed.entries:
                        logger.warning(f"{source['name']} ì—”íŠ¸ë¦¬ ì—†ìŒ")
                        return items
                    
                    for entry in feed.entries[:20]:
                        try:
                            title = entry.title
                            link = entry.link
                            
                            # ì¤‘ë³µ ì²´í¬
                            if self._is_duplicate(title, link):
                                continue
                            
                            # ì‹œê°„ ì¶”ì¶œ
                            pub_time = self._extract_time(entry, source['name'])
                            
                            # 24ì‹œê°„ í•„í„°
                            age_hours = (datetime.now(self.kst) - pub_time).total_seconds() / 3600
                            if age_hours > 24:
                                continue
                            
                            # í‚¤ì›Œë“œ í•„í„°
                            if not self._passes_keyword_filter(title):
                                continue
                            
                            # ë“±ë¡
                            self._register_news(title, link)
                            
                            items.append({
                                'id': f"{source['name']}_{link}",
                                'title': title,
                                'url': link,
                                'source': source['name'],
                                'market': source['market'],
                                'timestamp': datetime.now(),
                                'published_timestamp': pub_time.timestamp()
                            })
                            
                        except Exception as e:
                            logger.debug(f"RSS í•­ëª© ì˜¤ë¥˜: {e}")
                            continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} RSS ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_html(self, source):
        """HTML í¬ë¡¤ë§"""
        items = []
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(source['url'], headers=headers, timeout=10) as resp:
                    if resp.status != 200:
                        return items
                    
                    soup = BeautifulSoup(await resp.text(), 'html.parser')
                    
                    for card in soup.select('.card-list .card')[:15]:
                        try:
                            a = card.select_one('h3 a') or card.find('a')
                            if not a:
                                continue
                            
                            title = a.get_text(strip=True)
                            link = a['href']
                            
                            if not link.startswith('http'):
                                link = source['base_url'] + link
                            
                            if self._is_duplicate(title, link):
                                continue
                            
                            if not self._passes_keyword_filter(title):
                                continue
                            
                            pub_time = datetime.now(self.kst)
                            
                            self._register_news(title, link)
                            
                            items.append({
                                'id': f"{source['name']}_{link}",
                                'title': title,
                                'url': link,
                                'source': source['name'],
                                'market': source['market'],
                                'timestamp': datetime.now(),
                                'published_timestamp': pub_time.timestamp()
                            })
                            
                        except Exception as e:
                            logger.debug(f"HTML ì¹´ë“œ ì˜¤ë¥˜: {e}")
                            continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} HTML ì˜¤ë¥˜: {e}")
            return items
    
    def _extract_time(self, entry, source_name):
        """ì‹œê°„ ì¶”ì¶œ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                t = entry.published_parsed
                dt_naive = datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
                dt_kst = dt_naive + timedelta(hours=9)  # UTC â†’ KST
                return self.kst.localize(dt_kst)
            
            if hasattr(entry, 'published'):
                return self._parse_et(entry.published)
                
        except Exception as e:
            logger.debug(f"ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return datetime.now(self.kst)
    
    def _parse_et(self, time_str):
        """ET â†’ KST ë³€í™˜"""
        try:
            match = re.search(r'(\d{1,2}:\d{2})', time_str)
            if match and any(tz in time_str for tz in ['ET', 'EST', 'EDT']):
                h, m = map(int, match.group(1).split(':'))
                now = datetime.now()
                dt_naive = datetime(now.year, now.month, now.day, h, m)
                dt_kst = dt_naive + timedelta(hours=14)
                return self.kst.localize(dt_kst)
        except:
            pass
        
        return datetime.now(self.kst)
    
    def _is_duplicate(self, title, url):
        """ì¤‘ë³µ ì²´í¬ (URL + ì œëª© ìœ ì‚¬ë„ 85%)"""
        if url in self.seen_urls:
            return True
        
        for seen_title in self.seen_titles:
            if SequenceMatcher(None, title, seen_title).ratio() > 0.85:
                return True
        
        return False
    
    def _register_news(self, title, url):
        """ì¤‘ë³µ ë°©ì§€ ë“±ë¡"""
        self.seen_urls.add(url)
        self.seen_titles.append(title)
        
        if len(self.seen_titles) > 100:
            self.seen_titles.pop(0)
    
    def _passes_keyword_filter(self, title):
        """í‚¤ì›Œë“œ í•„í„° (Config.POSITIVE/NEGATIVE)"""
        title_lower = title.lower()
        
        has_positive = any(kw in title_lower for kw in Config.POSITIVE_KEYWORDS)
        has_negative = any(kw in title_lower for kw in Config.NEGATIVE_KEYWORDS)
        
        return has_positive and not has_negative
    
    async def process_news(self, news_item):
        """
        ë‰´ìŠ¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        ì¢…ëª© ì—†ì–´ë„ OK â†’ AIê°€ ìˆ˜í˜œì£¼ ì°¾ê¸°
        """
        try:
            # 1ì°¨: ë¹ ë¥¸ ì ìˆ˜
            is_promising = await self.ai.quick_score(news_item['title'], threshold=8.0)
            
            if not is_promising:
                return None
            
            # 2ì°¨: ìƒì„¸ ë¶„ì„
            analysis = await self.ai.analyze_news_signal(news_item)
            
            if not analysis or analysis['score'] < 8.5:
                return None
            
            # 3ì¤‘ ê²€ì¦
            verified = await self.verify_signals(analysis, news_item)
            
            if not verified:
                return None
            
            return {
                'news': news_item,
                'analysis': analysis,
                'verified': True,
                'verification_details': verified,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    async def verify_signals(self, analysis, news_item):
        """3ì¤‘ ê²€ì¦ (ìŠ¹ë¥  80%)"""
        verification = {
            'ai_score': analysis['score'],
            'checks_passed': [],
            'total_score': 0
        }
        
        # 1ì°¨: AI ì ìˆ˜
        if analysis['score'] >= 9.0:
            verification['total_score'] += 50
            verification['checks_passed'].append('AI ì´ˆê³ ì ìˆ˜')
        elif analysis['score'] >= 8.5:
            verification['total_score'] += 40
            verification['checks_passed'].append('AI ê³ ì ìˆ˜')
        else:
            return None
        
        # í™•ì‹¤ì„±
        if analysis.get('certainty') == 'confirmed':
            verification['total_score'] += 15
            verification['checks_passed'].append('í™•ì • ë‰´ìŠ¤')
        
        # 2ì°¨: ì‹œì¥ ë°˜ì‘
        verification['total_score'] += 10
        verification['checks_passed'].append('ì‹œì¥ ë¶„ì„')
        
        # 3ì°¨: ë‰´ìŠ¤ íƒ€ì…
        news_type = self._classify_news_type(news_item['title'])
        pattern_score = {
            'approval': 25,
            'earnings': 20,
            'contract': 20,
            'government': 15,
            'product': 15,
            'other': 5
        }.get(news_type, 5)
        
        verification['total_score'] += pattern_score
        verification['checks_passed'].append(f'íƒ€ì…: {news_type}')
        
        # ìµœì¢…: 80ì  ì´ìƒ
        if verification['total_score'] >= 80:
            return verification
        else:
            return None
    
    def _classify_news_type(self, title):
        """ë‰´ìŠ¤ íƒ€ì… ë¶„ë¥˜"""
        title_lower = title.lower()
        
        keywords = {
            'approval': ['ìŠ¹ì¸', 'approval', 'approved', 'fda'],
            'earnings': ['ì‹¤ì ', 'earnings', 'ì˜ì—…ì´ìµ'],
            'contract': ['ê³„ì•½', 'contract', 'ìˆ˜ì£¼'],
            'government': ['ì •ë¶€', 'government', 'subsidy'],
            'product': ['ì¶œì‹œ', 'launch', 'product'],
        }
        
        for news_type, words in keywords.items():
            if any(word in title_lower for word in words):
                return news_type
        
        return 'other'
    
    def cleanup_old_news(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if len(self.seen_urls) > 1000:
            self.seen_urls = set(list(self.seen_urls)[-500:])
        if len(self.seen_titles) > 100:
            self.seen_titles = self.seen_titles[-50:]
