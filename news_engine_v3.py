# -*- coding: utf-8 -*-
"""
News Engine v3.0 - Beast Mode (ì•¼ìˆ˜ ëª¨ë“œ)
- 5ëŒ€ì¥ ë‰´ìŠ¤ ì†ŒìŠ¤ (ë¯¸êµ­)
- ğŸ”¥ í•œêµ­ ë‰´ìŠ¤ ì†ŒìŠ¤ ëŒ€í­ í™•ì¥ (ë„¤ì´ë²„ ì†ë³´, ë§¤ê²½, í•œê²½, ì„œê²½)
- SEC 8-K
- curl_cffi ë³´ì•ˆ ìš°íšŒ
- KST ì‹œê°„ ì²˜ë¦¬
"""

import asyncio
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import feedparser
import pytz
from difflib import SequenceMatcher
import re
from curl_cffi.requests import AsyncSession

from ai_brain_v3 import AIBrainV3
from config import Config

logger = logging.getLogger(__name__)

class NewsEngineV3:
    def __init__(self, ai_brain):
        self.ai = ai_brain
        self.seen_urls = set()
        self.seen_titles = []
        
        # Timezone
        self.kst = pytz.timezone('Asia/Seoul')
        
        # ğŸ”¥ v3.0: ë‰´ìŠ¤ ì†ŒìŠ¤ ëŒ€í­ í™•ì¥
        self.sources = [
            # === ë¯¸êµ­ ë‰´ìŠ¤ (5ëŒ€ì¥ + SEC) ===
            {
                'name': 'PR Newswire',
                'type': 'rss',
                'url': 'https://www.prnewswire.com/rss/news-releases-list.rss',
                'market': 'US'
            },
            {
                'name': 'GlobeNewswire',
                'type': 'rss',
                'url': 'https://www.globenewswire.com/RssFeed/subjectcode/15-allcategories/feedTitle/GlobeNewswire%20-%20All%20Categories',
                'market': 'US'
            },
            {
                'name': 'Business Wire',
                'type': 'html',
                'url': 'https://www.businesswire.com/portal/site/home/news/',
                'pattern': r'/news/home/\d+/',
                'market': 'US'
            },
            {
                'name': 'Benzinga',
                'type': 'html',
                'url': 'https://www.benzinga.com/news',
                'pattern': r'/news/\d+/',
                'market': 'US'
            },
            
            # === ğŸ”¥ í•œêµ­ ë‰´ìŠ¤ ì†ŒìŠ¤ (v3.1.1 ìµœì¢…) ===
            {
                'name': 'ë„¤ì´ë²„ ì¦ê¶Œ ì†ë³´',
                'type': 'naver_breaking',
                'url': 'https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258',
                'market': 'KR'
            },
            {
                'name': 'ë§¤ì¼ê²½ì œ',
                'type': 'rss',
                'url': 'https://www.mk.co.kr/rss/30000001/',
                'market': 'KR'
            },
            {
                'name': 'í•œêµ­ê²½ì œ',
                'type': 'rss',
                'url': 'https://www.hankyung.com/feed/economy',
                'market': 'KR'
            },
            # ğŸ”§ v3.1.1: ì„œìš¸ê²½ì œ ì™„ì „ ì œê±° (RSS ì„œë¹„ìŠ¤ íì§€ë¨)
        ]
        
        # SEC 8-K ê³µì‹œ
        self.sec_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=8-K&company=&dateb=&owner=include&start=0&count=100&output=atom'
        
        logger.info("ğŸ“° News Engine v3.0 Beast Mode ì´ˆê¸°í™”")
    
    async def scan_all_sources(self):
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ë³‘ë ¬ ìŠ¤ìº” (curl_cffi)"""
        async with AsyncSession(impersonate="chrome110") as session:
            tasks = []
            
            for source in self.sources:
                if source['type'] == 'rss':
                    tasks.append(self._fetch_rss(session, source))
                elif source['type'] == 'html':
                    tasks.append(self._fetch_html(session, source))
                elif source['type'] == 'naver_breaking':
                    tasks.append(self._fetch_naver_breaking(session, source))
            
            tasks.append(self._fetch_sec(session))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            news_list = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    source_name = self.sources[i]['name'] if i < len(self.sources) else 'SEC 8-K'
                    logger.error(f"{source_name} ìŠ¤ìº” ì˜¤ë¥˜: {result}")
                elif result:
                    news_list.extend(result)
            
            news_list.sort(key=lambda x: x.get('published_timestamp', 0), reverse=True)
            
            logger.info(f"ğŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘: {len(news_list)}ê°œ (ë¯¸êµ­ 5ëŒ€ì¥ + í•œêµ­ 3ëŒ€ì¥ + SEC)")
            return news_list
    
    async def _fetch_rss(self, session, source):
        """RSS í”¼ë“œ ìŠ¤ìº” (ë¯¸êµ­/í•œêµ­ ê³µí†µ)"""
        items = []
        
        try:
            response = await session.get(source['url'], timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"{source['name']} RSS ì‹¤íŒ¨: {response.status_code}")
                # ğŸ”§ v3.1: 404 ë“± ì—ëŸ¬ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ê³„ì† ì§„í–‰
                return items
            
            # ğŸ”§ v3.1: RSS íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ê³„ì† ì§„í–‰
            try:
                feed = feedparser.parse(response.text)
            except Exception as e:
                logger.warning(f"{source['name']} RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
                return items
            
            if not feed.entries:
                logger.warning(f"{source['name']} ì—”íŠ¸ë¦¬ ì—†ìŒ")
                # ğŸ”§ v3.1: ì—”íŠ¸ë¦¬ ì—†ì–´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ê³„ì† ì§„í–‰
                return items
            
            for entry in feed.entries[:20]:
                try:
                    title = entry.title
                    link = entry.link
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    pub_time = self._extract_rss_time(entry)
                    
                    age_hours = (datetime.now(self.kst) - pub_time).total_seconds() / 3600
                    if age_hours > 24:
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"{source['name']}_{link}",
                        'title': title,
                        'url': link,
                        'source': source['name'],
                        'market': source['market'],
                        'type': 'news',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"RSS í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} RSS ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_naver_breaking(self, session, source):
        """
        ğŸ”¥ v3.0 ì‹ ê·œ: ë„¤ì´ë²„ ì¦ê¶Œ ì†ë³´ í¬ë¡¤ë§
        - íŠ¹ì§•ì£¼, ë‹¨ë…, ì†ë³´ ìš°ì„ 
        """
        items = []
        
        try:
            response = await session.get(source['url'], timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"{source['name']} ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                return items
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            news_items = soup.select('dl.newsList dd.articleSubject a')[:30]
            
            for item in news_items:
                try:
                    title = item.text.strip()
                    link = item.get('href', '')
                    
                    if not link.startswith('http'):
                        link = 'https://finance.naver.com' + link
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    # ğŸ”¥ íŠ¹ì§•ì£¼, ë‹¨ë…, ì†ë³´ ìš°ì„  ì²˜ë¦¬
                    is_priority = any(keyword in title for keyword in ['íŠ¹ì§•ì£¼', 'ë‹¨ë…', 'ì†ë³´', 'ê¸´ê¸‰'])
                    
                    if not is_priority and not self._passes_keyword_filter(title):
                        continue
                    
                    pub_time = datetime.now(self.kst)
                    
                    self._register_news(title, link)
                    
                    # ìš°ì„ ë„ ë†’ì€ ë‰´ìŠ¤ì— ë§ˆí‚¹
                    priority_tag = " ğŸ”¥" if is_priority else ""
                    
                    items.append({
                        'id': f"{source['name']}_{link}",
                        'title': title + priority_tag,
                        'url': link,
                        'source': source['name'],
                        'market': source['market'],
                        'type': 'news',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST'),
                        'is_priority': is_priority
                    })
                    
                except Exception as e:
                    logger.debug(f"ë„¤ì´ë²„ ì†ë³´ í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_html(self, session, source):
        """HTML í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (Business Wire, Benzinga)"""
        items = []
        
        try:
            response = await session.get(source['url'], timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"{source['name']} HTML ì‹¤íŒ¨: {response.status_code}")
                return items
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Business Wire íŠ¹ìˆ˜ ì²˜ë¦¬
            if source['name'] == 'Business Wire':
                news_items = soup.select('a.bwNewsList__link')[:20]
                
                for item in news_items:
                    try:
                        title = item.text.strip()
                        link = item.get('href', '')
                        
                        if not link.startswith('http'):
                            link = 'https://www.businesswire.com' + link
                        
                        if self._is_duplicate(title, link): continue
                        if not self._passes_keyword_filter(title): continue
                        
                        pub_time = datetime.now(self.kst)
                        
                        self._register_news(title, link)
                        
                        items.append({
                            'id': f"{source['name']}_{link}",
                            'title': title,
                            'url': link,
                            'source': source['name'],
                            'market': source['market'],
                            'type': 'news',
                            'timestamp': datetime.now(),
                            'published_timestamp': pub_time.timestamp(),
                            'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                        })
                    except Exception:
                        continue
                
                logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
                return items

            # Benzinga ë° ê¸°íƒ€ ì¼ë°˜ HTML ì²˜ë¦¬
            links = soup.find_all('a', href=re.compile(source['pattern']))
            
            for link_tag in links[:15]:
                try:
                    title = link_tag.get_text(strip=True)
                    link = link_tag.get('href')
                    
                    if not link.startswith('http'):
                        if source['name'] == 'Benzinga':
                            link = 'https://www.benzinga.com' + link
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    pub_time = datetime.now(self.kst)
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"{source['name']}_{link}",
                        'title': title,
                        'url': link,
                        'source': source['name'],
                        'market': source['market'],
                        'type': 'news',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"HTML ë§í¬ ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… {source['name']}: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"{source['name']} HTML ì˜¤ë¥˜: {e}")
            return items
    
    async def _fetch_sec(self, session):
        """SEC 8-K ê³µì‹œ í¬ë¡¤ë§"""
        items = []
        
        try:
            headers = {'User-Agent': 'StockBot/3.0 (admin@stockbot.com)'}
            response = await session.get(self.sec_url, headers=headers, timeout=20)
            
            if response.status_code != 200:
                logger.warning(f"SEC 8-K ì‹¤íŒ¨: {response.status_code}")
                return items
            
            soup = BeautifulSoup(response.text, 'xml')
            entries = soup.find_all('entry')
            
            for entry in entries[:30]:
                try:
                    title_tag = entry.find('title')
                    link_tag = entry.find('link')
                    updated_tag = entry.find('updated')
                    
                    if not title_tag or not link_tag:
                        continue
                    
                    title = title_tag.text.strip()
                    link = link_tag.get('href')
                    
                    title = f"[ê³µì‹œ] {title}"
                    
                    if self._is_duplicate(title, link):
                        continue
                    
                    pub_time = self._extract_sec_time(updated_tag)
                    
                    age_hours = (datetime.now(self.kst) - pub_time).total_seconds() / 3600
                    if age_hours > 24:
                        continue
                    
                    if not self._passes_keyword_filter(title):
                        continue
                    
                    self._register_news(title, link)
                    
                    items.append({
                        'id': f"SEC_{link}",
                        'title': title,
                        'url': link,
                        'source': 'SEC 8-K',
                        'market': 'US',
                        'type': 'filing',
                        'timestamp': datetime.now(),
                        'published_timestamp': pub_time.timestamp(),
                        'published_time_kst': pub_time.strftime('%Y-%m-%d %H:%M:%S KST')
                    })
                    
                except Exception as e:
                    logger.debug(f"SEC í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… SEC 8-K: {len(items)}ê°œ")
            return items
            
        except Exception as e:
            logger.error(f"SEC 8-K ì˜¤ë¥˜: {e}")
            return items
    
    def _extract_rss_time(self, entry):
        """RSS ë°œê°„ ì‹œê°„ íŒŒì‹± â†’ KST"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                t = entry.published_parsed
                dt_naive = datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
                dt_kst = dt_naive + timedelta(hours=9)
                return self.kst.localize(dt_kst)
            
            if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                t = entry.updated_parsed
                dt_naive = datetime(t.tm_year, t.tm_mon, t.tm_mday, t.tm_hour, t.tm_min, t.tm_sec)
                dt_kst = dt_naive + timedelta(hours=9)
                return self.kst.localize(dt_kst)
                
        except Exception as e:
            logger.debug(f"RSS ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return datetime.now(self.kst)
    
    def _extract_sec_time(self, updated_tag):
        """SEC ë°œê°„ ì‹œê°„ íŒŒì‹± â†’ KST"""
        try:
            if updated_tag:
                text = updated_tag.text.strip()
                dt_utc = datetime.fromisoformat(text.replace('Z', '+00:00'))
                dt_kst = dt_utc.astimezone(self.kst)
                return dt_kst
        except Exception as e:
            logger.debug(f"SEC ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return datetime.now(self.kst)
    
    def _is_duplicate(self, title, url):
        """ì¤‘ë³µ ì²´í¬ (URL + ì œëª© ìœ ì‚¬ë„)"""
        if url in self.seen_urls:
            return True
        
        for seen_title in self.seen_titles[-50:]:
            similarity = SequenceMatcher(None, title.lower(), seen_title.lower()).ratio()
            if similarity > 0.85:
                return True
        
        return False
    
    def _register_news(self, title, url):
        """ë‰´ìŠ¤ ë“±ë¡"""
        self.seen_urls.add(url)
        self.seen_titles.append(title)
        
        if len(self.seen_urls) > 2000:
            self.seen_urls.clear()
        if len(self.seen_titles) > 500:
            self.seen_titles = self.seen_titles[-250:]
    
    def _passes_keyword_filter(self, title):
        """í‚¤ì›Œë“œ í•„í„° (Config ê¸°ë°˜)"""
        title_upper = title.upper()
        
        # ì•…ì¬ í‚¤ì›Œë“œ ë¨¼ì € ì²´í¬
        for negative in Config.NEGATIVE_KEYWORDS:
            if negative.upper() in title_upper:
                return False
        
        # í˜¸ì¬ í‚¤ì›Œë“œ ì²´í¬
        for positive in Config.POSITIVE_KEYWORDS:
            if positive.upper() in title_upper:
                return True
        
        return False
